{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfDA3RBIzlfPztZcLjFLVp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2m-BCdlpeKax"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Profundizando en redes neuronales con Jax\n","\n","* En este notebook vamos a enfocarnos en hacer una clase más completa que permita entrenar una red neuronal de tamaño más arbitrario.\n","* Seguiremos utilizando el enfoque de hacerlo como en Scikit-learn."],"metadata":{"id":"_YK179_5eRHd"}},{"cell_type":"code","source":["# Importamos libre\n","from jax import numpy as jnp\n","from jax import grad\n","from jax import jacobian\n","from jax import random\n","from jax import vmap\n","from jax import value_and_grad\n","from jax import jit\n","from jax import nn\n","from jax import lax\n","import sys\n","import numpy as np"],"metadata":{"id":"dUmB3IqLeeiV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class JaxMLPClassifier():\n","  def __init__(self, units, epochs=10, lr=0.01, binary=True, n_classes=2, activation='relu', seed=0, batch_size=-1):\n","    self.key = random.key(seed)\n","    self.epochs = epochs\n","    self.lr = lr\n","    self.W = dict()\n","    self.batch_size = batch_size\n","    if activation == 'relu':\n","      self.activation = nn.relu\n","    elif activation == 'sigmoid':\n","      self.activation = nn.sigmoid\n","    else:\n","      self.activation = nn.tanh\n","\n","    if binary:\n","      self.units = units + [1]\n","    else:\n","      self.units = units + [n_classes]\n","\n","  def forward(self, W, X):\n","    n_layers = len(self.units)\n","    output = X\n","    for i in range(n_layers-1):\n","      #W = W[i]\n","      z = jnp.dot(output, jnp.transpose(W[i]))\n","      output = self.activation(z)\n","    z = jnp.dot(output, jnp.transpose(W[n_layers-1]))\n","    output = nn.sigmoid(z)\n","    return output\n","\n","  def predict(self, X):\n","    n_layers = len(self.units)\n","    output = X\n","    for i in range(n_layers-1):\n","      #W = W[i]\n","      z = jnp.dot(output, jnp.transpose(self.W[i]))\n","      output = self.activation(z)\n","    z = jnp.dot(output, jnp.transpose(self.W[n_layers-1]))\n","    output = nn.sigmoid(z)\n","    return output\n","\n","  def loss(self, W, X, y):\n","    yp = self.forward(W, X)\n","    #print(\"yp: \", jnp.min(yp))\n","    #print(\"y: \", y)\n","    #l = jnp.sum(jnp.pow(yp - y, 2))/y.shape[0]\n","    l = jnp.log(yp) * y + jnp.log(1-yp) * (1 - y) # entropia cruzada\n","    #print(\"min losses:\", jnp.max(l))\n","    #print(\"max losses:\", jnp.min(l))\n","    ls = -jnp.sum(l)/y.shape[0]\n","    return ls\n","\n","\n","  def fit(self, X, y):\n","    n_features = X.shape[1]\n","    n_samples = X.shape[0]\n","    n_layers = len(self.units) # +1 para tomar en cuenta la capa de salida\n","\n","    # Creamos las matrices de pesos.\n","    keys = random.split(self.key, n_layers)\n","    W = dict()\n","    for i in range(n_layers):\n","      n_units = self.units[i]\n","      W[i] = random.normal(keys[i], (n_units, n_features))\n","      n_features = self.units[i]\n","\n","    # calculamos el gradiente y calculamos en cada batch\n","    #val_grad_loss = jit(value_and_grad(self.loss, 0))\n","    val_grad_loss = value_and_grad(self.loss, 0)\n","\n","    # Entrenamos la red\n","    X_p = lax.slice(X, (0,0), (self.batch_size,X.shape[1]))\n","    y_p = lax.slice(y, (0,), (self.batch_size,))\n","    loss_val, grad_val = val_grad_loss(W, X_p, y_p)\n","    print(f\"Initial loss: {loss_val}\")\n","    for ep in range(self.epochs):\n","      if self.batch_size == -1:\n","        # Calcular gradiente\n","        loss_val, grad_w = val_grad_loss(W, X, y)\n","        print(f\"Epoch: {ep} - Loss: {loss_val}\")\n","        #print(\"Grad: \", grad_w)\n","\n","        # Actualizar pesos\n","        for i in range(n_layers):\n","          W[i] = W[i] - self.lr*grad_w[i]\n","      else:\n","        n_batches = int(jnp.floor(y.shape[0]/self.batch_size))\n","        #print(n_batches)\n","        avg_loss = 0\n","        for n in range(n_batches-1):\n","          i0 = n*self.batch_size\n","          i1 = (n+1)*self.batch_size\n","          #print(i0)\n","          #print(i1)\n","          X_p = lax.slice(X, (i0,0), (i1, X.shape[1]))\n","          y_p = lax.slice(y, (i0,), (i1,))\n","          loss_val, grad_w = val_grad_loss(W, X_p, y_p)\n","          avg_loss += loss_val\n","          for i in range(n_layers):\n","            W[i] = W[i] - self.lr*grad_w[i]\n","        print(f\"Epoch: {ep} - Loss: {avg_loss/n_batches}\")\n","    # Guardamos pesos\n","    self.W = W"],"metadata":{"id":"2d9_xfk4fHU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Probamos un entrenamiento\n","X = jnp.array([[1.0, 3.5],[-0.5, 0.6], [-0.5, 1.3]])\n","#y = jnp.array([[1],[0],[0]])\n","y = jnp.array([[1.0],[0.0],[0.0]])\n"],"metadata":{"id":"bgrKMUj8sBfH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creamos y entrenamos modelo\n","model = JaxMLPClassifier([8,8], lr=0.01, epochs=1, activation='relu')\n","model.fit(X,y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EupBzbymslMM","executionInfo":{"status":"ok","timestamp":1721627290173,"user_tz":360,"elapsed":1131,"user":{"displayName":"Angel Alejandro Maldonado Ramírez","userId":"05170948916545730550"}},"outputId":"f5bc199d-1431-42d0-a4cf-6eb4eef261b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial loss: 2.5114800930023193\n","Epoch: 0 - Loss: 2.5114800930023193\n","Epoch: 1 - Loss: 2.1464388370513916\n","Epoch: 2 - Loss: 1.8001383543014526\n","Epoch: 3 - Loss: 1.4716453552246094\n","Epoch: 4 - Loss: 1.162377119064331\n","Epoch: 5 - Loss: 0.8781678676605225\n","Epoch: 6 - Loss: 0.6312857270240784\n","Epoch: 7 - Loss: 0.4375635087490082\n","Epoch: 8 - Loss: 0.3042278587818146\n","Epoch: 9 - Loss: 0.2213926464319229\n","Epoch: 10 - Loss: 0.17141039669513702\n","Epoch: 11 - Loss: 0.14035014808177948\n","Epoch: 12 - Loss: 0.1200738325715065\n","Epoch: 13 - Loss: 0.10616356879472733\n","Epoch: 14 - Loss: 0.09619425982236862\n","Epoch: 15 - Loss: 0.08877945691347122\n","Epoch: 16 - Loss: 0.08308900147676468\n","Epoch: 17 - Loss: 0.07860378175973892\n","Epoch: 18 - Loss: 0.07498599588871002\n","Epoch: 19 - Loss: 0.07200868427753448\n","Epoch: 20 - Loss: 0.06951439380645752\n","Epoch: 21 - Loss: 0.06739137321710587\n","Epoch: 22 - Loss: 0.06555844843387604\n","Epoch: 23 - Loss: 0.06395541876554489\n","Epoch: 24 - Loss: 0.0625370591878891\n","Epoch: 25 - Loss: 0.06126868724822998\n","Epoch: 26 - Loss: 0.06012343242764473\n","Epoch: 27 - Loss: 0.0590803287923336\n","Epoch: 28 - Loss: 0.05812263488769531\n","Epoch: 29 - Loss: 0.05723698064684868\n","Epoch: 30 - Loss: 0.05641263723373413\n","Epoch: 31 - Loss: 0.055640846490859985\n","Epoch: 32 - Loss: 0.054914262145757675\n","Epoch: 33 - Loss: 0.05422714352607727\n","Epoch: 34 - Loss: 0.05357438325881958\n","Epoch: 35 - Loss: 0.05295189097523689\n","Epoch: 36 - Loss: 0.05235624313354492\n","Epoch: 37 - Loss: 0.051784515380859375\n","Epoch: 38 - Loss: 0.05123404040932655\n","Epoch: 39 - Loss: 0.050702959299087524\n","Epoch: 40 - Loss: 0.050189245492219925\n","Epoch: 41 - Loss: 0.04969140887260437\n","Epoch: 42 - Loss: 0.04920807480812073\n","Epoch: 43 - Loss: 0.04873804748058319\n","Epoch: 44 - Loss: 0.04828030988574028\n","Epoch: 45 - Loss: 0.0478370226919651\n","Epoch: 46 - Loss: 0.047414060682058334\n","Epoch: 47 - Loss: 0.047001246362924576\n","Epoch: 48 - Loss: 0.046597737818956375\n","Epoch: 49 - Loss: 0.04620303586125374\n"]}]},{"cell_type":"code","source":["yp = model.predict(X)\n","yp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y3xpRbcgsp4K","executionInfo":{"status":"ok","timestamp":1721627294322,"user_tz":360,"elapsed":202,"user":{"displayName":"Angel Alejandro Maldonado Ramírez","userId":"05170948916545730550"}},"outputId":"6a2060f4-aac0-4654-a3e4-bb3713e34e95"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Array([[0.956443  ],\n","       [0.0394823 ],\n","       [0.05127172]], dtype=float32)"]},"metadata":{},"execution_count":212}]},{"cell_type":"code","source":["np.log(1.5212246393114803e-11)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ln2RQHy_tLJL","executionInfo":{"status":"ok","timestamp":1721627817649,"user_tz":360,"elapsed":203,"user":{"displayName":"Angel Alejandro Maldonado Ramírez","userId":"05170948916545730550"}},"outputId":"79227a95-a272-4e9e-a22c-4d3a7b1e4cf5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-24.908920328707698"]},"metadata":{},"execution_count":232}]},{"cell_type":"markdown","source":["## Comparación Scikit-learn vs jax\n","\n","* Vamos a tomar un dataset existente y comparar en términos de error y tiempo de entrenamiento.\n"],"metadata":{"id":"UapxJRn10qJL"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"QzYf57gq0sc7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cargamos el dataset\n","X, y = load_breast_cancer(return_X_y=True)"],"metadata":{"id":"JAGImsFB1ZCb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# normalizamos los datos\n","scaler = StandardScaler()\n","scaler.fit(X)\n","X_norm = scaler.transform(X)\n","#X_norm"],"metadata":{"id":"wU5h4oao25Pp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convertimos a formato de Jax\n","X_jax = jnp.array(X_norm)\n","y_jax = jnp.array(y)"],"metadata":{"id":"84RbXzca1lsy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jax_model = JaxMLPClassifier([4,4], lr=0.005, epochs=100, activation='tanh', seed=12, batch_size=64)\n","jax_model.fit(X_jax,y_jax)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAIDC04s1meC","executionInfo":{"status":"ok","timestamp":1721630679998,"user_tz":360,"elapsed":12442,"user":{"displayName":"Angel Alejandro Maldonado Ramírez","userId":"05170948916545730550"}},"outputId":"065d0958-adfa-4205-9f51-a9b0952b62fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial loss: 82.90943908691406\n","Epoch: 0 - Loss: 57.80400466918945\n","Epoch: 1 - Loss: 43.465232849121094\n","Epoch: 2 - Loss: 40.94175338745117\n","Epoch: 3 - Loss: 39.86909484863281\n","Epoch: 4 - Loss: 39.35636901855469\n","Epoch: 5 - Loss: 39.08998107910156\n","Epoch: 6 - Loss: 38.936973571777344\n","Epoch: 7 - Loss: 38.83965301513672\n","Epoch: 8 - Loss: 38.77171325683594\n","Epoch: 9 - Loss: 38.72043228149414\n","Epoch: 10 - Loss: 38.679256439208984\n","Epoch: 11 - Loss: 38.64460754394531\n","Epoch: 12 - Loss: 38.61449432373047\n","Epoch: 13 - Loss: 38.587677001953125\n","Epoch: 14 - Loss: 38.56344223022461\n","Epoch: 15 - Loss: 38.54129409790039\n","Epoch: 16 - Loss: 38.520835876464844\n","Epoch: 17 - Loss: 38.501705169677734\n","Epoch: 18 - Loss: 38.48351287841797\n","Epoch: 19 - Loss: 38.46590805053711\n","Epoch: 20 - Loss: 38.4486083984375\n","Epoch: 21 - Loss: 38.43164825439453\n","Epoch: 22 - Loss: 38.41552734375\n","Epoch: 23 - Loss: 38.40092468261719\n","Epoch: 24 - Loss: 38.38808822631836\n","Epoch: 25 - Loss: 38.37671661376953\n","Epoch: 26 - Loss: 38.36634063720703\n","Epoch: 27 - Loss: 38.35655212402344\n","Epoch: 28 - Loss: 38.34708786010742\n","Epoch: 29 - Loss: 38.33773422241211\n","Epoch: 30 - Loss: 38.32825469970703\n","Epoch: 31 - Loss: 38.318382263183594\n","Epoch: 32 - Loss: 38.30775833129883\n","Epoch: 33 - Loss: 38.296260833740234\n","Epoch: 34 - Loss: 38.284732818603516\n","Epoch: 35 - Loss: 38.2747917175293\n","Epoch: 36 - Loss: 38.26677322387695\n","Epoch: 37 - Loss: 38.26003646850586\n","Epoch: 38 - Loss: 38.25405502319336\n","Epoch: 39 - Loss: 38.24861145019531\n","Epoch: 40 - Loss: 38.24354553222656\n","Epoch: 41 - Loss: 38.23876190185547\n","Epoch: 42 - Loss: 38.234214782714844\n","Epoch: 43 - Loss: 38.229888916015625\n","Epoch: 44 - Loss: 38.2257194519043\n","Epoch: 45 - Loss: 38.221702575683594\n","Epoch: 46 - Loss: 38.21781921386719\n","Epoch: 47 - Loss: 38.21406173706055\n","Epoch: 48 - Loss: 38.210411071777344\n","Epoch: 49 - Loss: 38.206871032714844\n","Epoch: 50 - Loss: 38.20341491699219\n","Epoch: 51 - Loss: 38.200042724609375\n","Epoch: 52 - Loss: 38.19676208496094\n","Epoch: 53 - Loss: 38.19355773925781\n","Epoch: 54 - Loss: 38.19042205810547\n","Epoch: 55 - Loss: 38.18736267089844\n","Epoch: 56 - Loss: 38.18436050415039\n","Epoch: 57 - Loss: 38.18141555786133\n","Epoch: 58 - Loss: 38.17853927612305\n","Epoch: 59 - Loss: 38.17570877075195\n","Epoch: 60 - Loss: 38.17292785644531\n","Epoch: 61 - Loss: 38.170204162597656\n","Epoch: 62 - Loss: 38.16749572753906\n","Epoch: 63 - Loss: 38.16486358642578\n","Epoch: 64 - Loss: 38.162254333496094\n","Epoch: 65 - Loss: 38.15968322753906\n","Epoch: 66 - Loss: 38.157135009765625\n","Epoch: 67 - Loss: 38.15462112426758\n","Epoch: 68 - Loss: 38.152137756347656\n","Epoch: 69 - Loss: 38.149688720703125\n","Epoch: 70 - Loss: 38.14725875854492\n","Epoch: 71 - Loss: 38.14485168457031\n","Epoch: 72 - Loss: 38.14247131347656\n","Epoch: 73 - Loss: 38.14011001586914\n","Epoch: 74 - Loss: 38.13777542114258\n","Epoch: 75 - Loss: 38.135459899902344\n","Epoch: 76 - Loss: 38.1331672668457\n","Epoch: 77 - Loss: 38.13088607788086\n","Epoch: 78 - Loss: 38.128631591796875\n","Epoch: 79 - Loss: 38.12639617919922\n","Epoch: 80 - Loss: 38.12417221069336\n","Epoch: 81 - Loss: 38.121986389160156\n","Epoch: 82 - Loss: 38.11981201171875\n","Epoch: 83 - Loss: 38.11766052246094\n","Epoch: 84 - Loss: 38.11552810668945\n","Epoch: 85 - Loss: 38.11341857910156\n","Epoch: 86 - Loss: 38.1113395690918\n","Epoch: 87 - Loss: 38.109283447265625\n","Epoch: 88 - Loss: 38.10723876953125\n","Epoch: 89 - Loss: 38.105220794677734\n","Epoch: 90 - Loss: 38.10322570800781\n","Epoch: 91 - Loss: 38.101253509521484\n","Epoch: 92 - Loss: 38.099300384521484\n","Epoch: 93 - Loss: 38.09736633300781\n","Epoch: 94 - Loss: 38.095462799072266\n","Epoch: 95 - Loss: 38.09357833862305\n","Epoch: 96 - Loss: 38.09170913696289\n","Epoch: 97 - Loss: 38.08986282348633\n","Epoch: 98 - Loss: 38.08804702758789\n","Epoch: 99 - Loss: 38.086246490478516\n"]}]},{"cell_type":"code","source":["yp_jax = jax_model.predict(X_jax)\n","yp = np.array(yp_jax.tolist())\n","preds = np.where(yp>0.5, 1, 0)\n","accuracy_score(y, preds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPenmJ6o2o88","executionInfo":{"status":"ok","timestamp":1721630657868,"user_tz":360,"elapsed":186,"user":{"displayName":"Angel Alejandro Maldonado Ramírez","userId":"05170948916545730550"}},"outputId":"331d1216-3784-46b7-90f4-6d37783b3724"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6274165202108963"]},"metadata":{},"execution_count":310}]},{"cell_type":"code","source":[],"metadata":{"id":"cjdvQlYLCg-Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Siguientes pasos:\n","\n","* Parece que para problemas reales mi implementación ya está batallando.\n","* Podría ser por el batch size y el método de aprendizaje. Parece que el gradiente descendiente no es muy bueno para manejar estos casos. Tal vez haya que implementar algo más sofisticado.\n","* Vemos que también es muy sensible a los parámetros que elijamos.\n","* Aquí es donde cobra relevanvia métodos como Adam y RMSProp."],"metadata":{"id":"sEtkmjYEDepH"}},{"cell_type":"code","source":[],"metadata":{"id":"JNuBuSEwD0cd"},"execution_count":null,"outputs":[]}]}